import pandas as pd
import os, datetime
import json

from helpers import get_newest_non_empty_dir_in_dir

def read_data_eval_orig():
    # Read report tables to visualize
    path_eval_orig = "../diagnosis_predictor_data/reports/evaluate_original_models/"
    dir_eval_orig_all = get_newest_non_empty_dir_in_dir(path_eval_orig, ["first_assessment_to_drop", # several assessments were used as opposed to one single assessment
                                                                    "only_free_assessments__0",
                                                                    #"debug_mode__False"
                                                                    ])
    dir_eval_orig_free = get_newest_non_empty_dir_in_dir(path_eval_orig, ["first_assessment_to_drop", # several assessments were used as opposed to one single assessment
                                                                    "only_free_assessments__1",
                                                                    #"debug_mode__False"
                                                                    ])
    
    return dir_eval_orig_all, dir_eval_orig_free

def read_data_eval_subsets():

    path_eval_subsets = "../diagnosis_predictor_data/reports/evaluate_models_on_feature_subsets/"
    dir_eval_subsets_all = get_newest_non_empty_dir_in_dir(path_eval_subsets, ["first_assessment_to_drop", # several assessments were used as opposed to one single assessment
                                                                  "only_free_assessments__0",
                                                                  #"debug_mode__False"
                                                                  ])
    dir_eval_subsets_free = get_newest_non_empty_dir_in_dir(path_eval_subsets, ["first_assessment_to_drop", # several assessments were used as opposed to one single assessment
                                                                  "only_free_assessments__1",
                                                                  #"debug_mode__False"
                                                                  ])

    return dir_eval_subsets_all, dir_eval_subsets_free

def read_data_make_ds():
    
    path = "../diagnosis_predictor_data/reports/create_datasets/"
    dir_all = get_newest_non_empty_dir_in_dir(path, ["first_assessment_to_drop", # several assessments were used as opposed to one single assessment
                                                        "only_free_assessments__0",
                                                        #"debug_mode__False"
                                                        ], 
                                                        "dataset_stats.csv") # Need to check if this file was created (some dirs were generated by
                                                                            # running the script with only_assessment_distribution=1, which does not create this file)
    dir_free = get_newest_non_empty_dir_in_dir(path, ["first_assessment_to_drop", # several assessments were used as opposed to one single assessment
                                                        "only_free_assessments__1",
                                                        #"debug_mode__False"
                                                        ],
                                                        "dataset_stats.csv")
    
    return dir_all, dir_free

def make_df_to_plot_eval_orig(dir_eval_orig_all, dir_eval_orig_free):
    # Read ROC AUC on all features on test set and for only healthy controls, for all assessments and free assessments
    eval_orig_all_df = pd.read_csv(dir_eval_orig_all + "performance_table_all_features.csv", index_col=0)
    eval_orig_free_df = pd.read_csv(dir_eval_orig_free + "performance_table_all_features.csv", index_col=0)

    # Each row is diagnosis (index), plot ROC AUC column and ROC AUC healthy controls column. 
    # Plot both for all assessments and free assessments on the same plot, add number of positive examples to each diagnosis
    eval_orig_all_df = eval_orig_all_df[["ROC AUC", "ROC AUC Mean CV", "ROC AUC Healthy Controls", "# of Positive Examples"]]
    eval_orig_free_df = eval_orig_free_df[["ROC AUC", "ROC AUC Mean CV", "ROC AUC Healthy Controls", "# of Positive Examples"]]
    eval_orig_all_df.columns = ["AUC all features all assessments", "AUC CV all features all assessments", "AUC all features healthy controls all assessments", "# of Positive Examples all assessments"]
    eval_orig_free_df.columns = ["AUC all features free assessments", "AUC CV all features free assessments", "AUC all features healthy controls free assessments", "# of Positive Examples free assessments"]
    eval_orig_df = eval_orig_all_df.merge(eval_orig_free_df, left_index=True, right_index=True).sort_values(by="AUC CV all features all assessments", ascending=False)

    return eval_orig_df

def make_df_to_plot_eval_subsets(dir_eval_subsets_all, dir_eval_subsets_free):
    df_opt_features_all = pd.read_csv(dir_eval_subsets_all + "auc-sens-spec-on-subsets-test-set-optimal-threshold-optimal-nb-features.csv", index_col=1)
    df_opt_features_free = pd.read_csv(dir_eval_subsets_free + "auc-sens-spec-on-subsets-test-set-optimal-threshold-optimal-nb-features.csv", index_col=1)

    df_manual = pd.read_csv("output/manual_scoring_analysis/manual_subsale_scores_vs_ml.csv", index_col=0)

    # Make one dataset with all info
    df_opt_features_all = df_opt_features_all[["AUC", "Number of features"]]
    df_opt_features_all.columns = ["AUC optimal features all assessments", "Optimal # of features all assessments"]
    df_opt_features_free = df_opt_features_free[["AUC", "Number of features"]]
    df_opt_features_free.columns = ["AUC optimal features free assessments", "Optimal # of features free assessments"]
    df_opt_features = df_opt_features_all.merge(df_opt_features_free, left_index=True, right_index=True)

    # Add manual scoring
    df_opt_features = df_opt_features.merge(df_manual, left_index=True, right_index=True).sort_values(by="Best subscale score", ascending=False)

    return df_opt_features

def make_df_ds_stats(dir_all, dir_free):
    
    ds_stats_df_all_assessments = pd.read_csv(dir_all + "dataset_stats.csv", index_col=0)
    ds_stats_df_free_assessments = pd.read_csv(dir_free + "dataset_stats.csv", index_col=0)

    print(ds_stats_df_all_assessments)

    ds_stats_df_all_assessments = ds_stats_df_all_assessments.T[["n_rows_full_ds", "n_input_cols"]]
    ds_stats_df_all_assessments.columns = ["# of rows full dataset all assessments", "# input features all assessments"]

    ds_stats_df_free_assessments = ds_stats_df_free_assessments.T[["n_rows_full_ds", "n_input_cols"]]
    ds_stats_df_free_assessments.columns = ["# of rows full dataset free assessments", "# input features free assessments"]

    #Merge
    ds_stats_df = ds_stats_df_all_assessments.merge(ds_stats_df_free_assessments, left_index=True, right_index=True)

    return ds_stats_df


def main():
    dir_eval_orig_all, dir_eval_orig_free = read_data_eval_orig()
    dir_eval_subsets_all, dir_eval_subsets_free = read_data_eval_subsets()
    dir_make_ds_all, dir_make_ds_free = read_data_make_ds()

    print("Reading reports from: ", dir_eval_orig_all, dir_eval_orig_free)
    print("\nReading reports from: ", dir_eval_subsets_all, dir_eval_subsets_free)
    print("\nReading reports from: ", dir_make_ds_all, dir_make_ds_free)

    eval_orig_df = make_df_to_plot_eval_orig(dir_eval_orig_all, dir_eval_orig_free)
    eval_subsets_df = make_df_to_plot_eval_subsets(dir_eval_subsets_all, dir_eval_subsets_free)
    ds_stats_df = make_df_ds_stats(dir_make_ds_all, dir_make_ds_free)
    
    compare_orig_vs_subsets_df = eval_orig_df.merge(eval_subsets_df, left_index=True, right_index=True)
    # Add total # of features and examples to compare_orig_vs_subsets_df
    for col in ds_stats_df.columns:
        compare_orig_vs_subsets_df[col] = ds_stats_df[col].values[0]

    eval_orig_df.to_csv("output/eval_orig.csv")
    eval_subsets_df.to_csv("output/eval_subsets.csv")
    compare_orig_vs_subsets_df.to_csv("output/compare_orig_vs_subsets.csv")


if __name__ == "__main__":
    main()