# HBN-scripts

Scripts to format output of the `diagnosis_predictor` for the paper

- `count_transdiag_items.py` calculates for how many diagnoses each item appears in each subset. Rows: item, colsumns: subset, values: count
- `score_manually.py` calculates AUROC of each subscale and total score from HBN for each diagnosis, and prints it alongside AUROC from my model on the number of features equal to the number of items in the best performing subscale (e.g. if ASSQ_Total has the highest AUROC for ASD, and it contains 27 items, I print AUROC of my model at 27 items). I also print the number of items my model needs to reach the performance of the best subscale. 
- `check_what_improves_LD.py` makes a table comparing scores for LD with and without NIH Scores, Conners, and test-based diagnoses. To generate the data use [`diagnosis-predictor`](https://github.com/charlie42/diagnosis-predictor) repo and [readme](https://docs.google.com/document/d/1xQaHDSa7YKXsVrUrfFXuX_sBycES0fT4BSv3XKtyL0M/edit)
- `make_final_tables.py` aggregates data from other scripts and the _data repos generated by the models and formats them into tables for the paper and plotting
- `make_plots.py` creates plots for the paper based on the tables created in `make_final_tables.py`
- `misc` folder contains other useful scripts that don't generate pretty tables
- `helpers.py` contain useful functions used in other scripts